# -*- coding: utf-8 -*-
#!/usr/bin/env python3
#Created on Tue Mar 10 15:42:46 2020
#@author: Norbert MoldovÃ¡n

import os.path, pandas as pd, argparse, pysam, subprocess


parser = argparse.ArgumentParser()
parser.add_argument("--input_path", "-i", dest="input_path", type=str, 
                    required=True, help="The path to the folder generated by LoRTIA. " 
                    "Must contain three files with the suffixes: tr.bam, tr.gff3 and tr.tsv")
parser.add_argument("--genome_path", "-g", dest="genome_path", type=str, 
                    required=True, help="The path to the .fasta containing the genome file. "
                    "Make sure it does not contain a \r line breaks!")
parser.add_argument("--out_path", "-o", dest="out_path", type=str, 
                    required=True, help="The path to the output folder.")
args = parser.parse_args()

gffCols = ["contig",
           "source",
           "feature",
           "start",
           "end",
           "readCount",
           "strand",
           "cds",
           "name",
           "id",
           "parent",
           "refSeq",
           "conSeq"]
tsvCols = ["tsvID",
           "gffID",
           "count"]
refCols =["ID",
          "refSeq"]

### Checks if output folder exists. If not, creates it.
def DirManager():
    if not os.path.exists(args.out_path):
        os.makedirs(args.out_path)
    if not os.path.exists("".join((args.out_path,"tmp/"))):
        os.makedirs("".join((args.out_path,"tmp/refSeq/")))
        os.makedirs("".join((args.out_path,"tmp/fa/")))
        os.makedirs("".join((args.out_path,"tmp/bam/")))
        os.makedirs("".join((args.out_path,"tmp/vcf/")))
        os.makedirs("".join((args.out_path,"tmp/consensus/")))
        
def ConsensusMaker():
    print("Creating consensus...")
    for files in os.walk("".join((args.out_path,"tmp/refSeq/"))):  
        for filename in files[2]:
            if filename.endswith(".fa"):
                clean_filename=filename.split(".fa")
                
                #Map sequences to the apropriate genomic segment
                mimap_com = 'minimap2 -ax map-ont -Y -v 0 {}tmp/refSeq/{}.fa {}tmp/fa/{}_tr.fa | samtools sort -o {}tmp/bam/{}.bam'.format(args.out_path, clean_filename[0], args.out_path, clean_filename[0], args.out_path, clean_filename[0])
                subprocess.run(mimap_com, shell=True)
                
                #Creating consensus from the mappings
                bcftools_com = 'bcftools mpileup -Ou -f {}tmp/refSeq/{}.fa {}tmp/bam/{}.bam | bcftools call -mv -Oz -o {}tmp/vcf/{}.vcf'.format(args.out_path, clean_filename[0], args.out_path, clean_filename[0], args.out_path, clean_filename[0])
                bcfindex_com = 'bcftools index {}tmp/vcf/{}.vcf'.format(args.out_path, clean_filename[0])
                bcfconsensus_com = 'cat {}tmp/refSeq/{}.fa | bcftools consensus {}tmp/vcf/{}.vcf > {}tmp/consensus/{}.fa'.format(args.out_path, clean_filename[0], args.out_path, clean_filename[0], args.out_path, clean_filename[0])
                subprocess.run(bcftools_com, shell=True)
                subprocess.run(bcfindex_com, shell=True)
                subprocess.run(bcfconsensus_com, shell=True)
                

def FileLoader():
    print("Loading annotation files... \n")
    
    gffDf = pd.DataFrame(columns=gffCols)
    tsvDf = pd.DataFrame()
    dataDf = pd.DataFrame(columns=gffCols)
    refSeqDf = pd.DataFrame()
    n = False
    
    for files in os.walk(args.input_path):  
        for filename in files[2]:
            if filename.endswith("tr.gff3"):
                clean_filename=filename.split("tr.gff3")
                gffDf = pd.read_csv("".join((files[0], clean_filename[0],"tr.gff3")),
                                       names=gffCols, sep = "\t", comment="#")
                 
                #Clarify transcript IDs and parents
                nameList = gffDf["name"].str.split(";")
                gffDf.id = list(item[0] for item in nameList)
                nameList = gffDf["id"].str.split("=")
                gffDf.id = list(item[1] for item in nameList)
                
                nameList = gffDf["name"].str.split(";")
                gffDf.parent = list(item[1] for item in nameList)
                nameList = gffDf["parent"].str.split("=")
                gffDf.parent = list(item[1] for item in nameList)
                
                #Extract reference sequences to a .tsv file.
                bedTools_com = 'bedtools getfasta -fi {} -bed {}{}tr.gff3 -fo {}{}refSeq.tsv -s -tab'.format(args.genome_path, files[0], clean_filename[0], args.out_path, clean_filename[0])
                subprocess.run(bedTools_com, shell=True)
                refSeqDf = pd.read_csv("".join((args.out_path,clean_filename[0],"refSeq.tsv")), names = refCols, sep = "\t")
                gffDf.refSeq = refSeqDf.refSeq
                
                #Merge exon refSeqs of polyexonic transcripts
                parSet = set(gffDf.parent)
                #print(gffDf)
                for p in parSet:
                    gffDf.loc[gffDf.id == p, "parent"] = ""
                    if p in (gffDf.parent).tolist():
                        
                        gffDf.loc[gffDf.id == p, "refSeq"] = "".join((gffDf[gffDf.parent == p].refSeq))
                        gffDf.loc[gffDf.parent == p, "parent"] = ""
                
                #Keep only the mRNA annotations. 
                gffDf = gffDf[gffDf.feature == "mRNA"]
                gffDf = gffDf.reset_index(drop = True)
                
            elif filename.endswith("tr.tsv"):
                clean_filename = filename.split("tr.tsv")
                tsvDf = pd.read_csv("".join((files[0], clean_filename[0], "tr.tsv")),
                                       names= tsvCols, sep = "\t", comment="#")
                
            elif filename.endswith("tr.bam"):
                clean_filename=filename.split("tr.bam")
                pysam.view("-o","".join((files[0], clean_filename[0],"tr.sam")),
                           "".join((files[0], clean_filename[0],"tr.bam")),catch_stdout=False)
    
    dataDf = gffDf.copy()
    for i,j in tsvDf.iterrows():
        fa_com = '''grep -w "tr:Z:{}" {}{}tr.sam | grep -v ^@ | awk '{{print ">"$1"\\n"$10}}' > {}tmp/fa/{}_tr.fa'''.format(j["tsvID"], files[0], clean_filename[0], args.out_path, j["tsvID"].replace("(","").replace(")","").replace("'","").replace(", ","_"))
        subprocess.run(fa_com, shell=True)
        dataDf.at[dataDf.id == j["gffID"],"tsvID"] = j["tsvID"].replace("(","").replace(")","").replace("'","").replace(", ","_")
        
        # Create reference sequence .fa files.
        if not dataDf[dataDf.id == j["gffID"]]["id"].empty:
            f = open("".join((args.out_path,"tmp/refSeq/",j["tsvID"].replace("(","").replace(")","").replace("'","").replace(", ","_"),".fa")),"w")
            f.write("".join((">",dataDf[dataDf.id == j["gffID"]]["id"].tolist()[0],"\n",dataDf[dataDf.id == j["gffID"]]["refSeq"].tolist()[0])))
            f.close()
        elif not n:
            print("Warning! Some transcripts may be missing from the tr.gff")
            n = True
    
    return dataDf

def ResultCreator(dataDf):
    
    for files in os.walk("".join((args.out_path,"tmp/consensus/"))):  
        for filename in files[2]:
            if filename.endswith(".fa"):
                clean_filename=filename.split(".fa")
                
                #Add sequences to the conSeq column and output a final .tsv file
                with open("".join((args.out_path,"tmp/consensus/",clean_filename[0],".fa")), 'r') as file:
                    f = "".join((file.readlines()[1:])).replace('\n', '')              
                dataDf.loc[dataDf.tsvID == clean_filename[0], "conSeq"] = f
                
    dataDf.to_csv("".join((args.out_path,"Transcripts.tsv")), sep = "\t", index=False,
                  columns=["contig",
                           "feature",
                           "start",
                           "end",
                           "readCount",
                           "strand",
                           "name",
                           "id",
                           "refSeq",
                           "conSeq"])
    
    #Merge .fa files and output a transcript reference
    cat_com = 'cat {}*.fa > {}RefTranscriptome.fa'.format("".join((args.out_path,"tmp/consensus/")), args.out_path)
    subprocess.run(cat_com, shell=True)
                
def Main():
   
   DirManager()
   dataDf = FileLoader()
   ConsensusMaker()
   ResultCreator(dataDf)

if __name__ == "__main__":
	Main()	

