#!/usr/bin/env python3
"""
Created on Thu Apr  2 08:30:55 2020

@author: Norbert MoldovÃ¡n

Modified on Jun 4 2020
Version 0.2
"""

import os.path, pandas as pd, argparse, statistics
from multiprocessing import Pool
from functools import partial

parser = argparse.ArgumentParser()
parser.add_argument("--input_path", "-i", dest="input_path", type=str, 
                    required=True, help="The path to the folder generated by LoRTIA. ")
parser.add_argument("--out_path", "-o", dest="out_path", type=str, 
                    required=True, help="The path to the output folder.")
parser.add_argument("--compare", "-c", dest="doCompare", action="store_true",
                    required=False, default=False, help="Compare TSS, TES and intron isoforms of the control and experimental group.\n"
                                                        "Requires the presence of at least one gff3 with a suffix of _control in the input directory!\n"
                                                        "Default: False")
parser.add_argument("--wobble", "-w", dest="w", type=int, 
                    required=False, default=0, help="+/- interval in which TSSs and TESs need to be searched. Not effecting splice junctions! Default: 0")
parser.add_argument("--suffix", "-s", dest="suffix", type=str, 
                    required=False, default="_control", help="The suffix of the control group's gff3. Default: '_control'")
args = parser.parse_args()

gffCols = ["contig",
           "source",
           "feature",
           "start",
           "end",
           "readCount",
           "strand",
           "cds",
           "name",
           "id",
           "parent",
           "seqLen",
           "exonCount",
           "exonComp"]
tsvCols = ["contig",
            "feature",
            "start",
            "end",
            "readCount",
            "strand",
            "name",
            "id",
            "seqLen",
            "exonCount",
            "exonComp"]
def DirManager():
    if not os.path.exists(args.out_path):
        os.makedirs(args.out_path)

def Stat():
    print("Calculating statistics...")
    
    tsvDf = pd.DataFrame()
    
    for files in os.walk(args.out_path):  
        for filename in files[2]:
            if filename.endswith("_trLen.tsv"):
                clean_filename=filename.split("_trLen.tsv")
                tsvDf = pd.read_csv("".join((files[0], clean_filename[0],"_trLen.tsv")),
                                       sep = "\t")
                
                tsvDf.seqLen = pd.to_numeric(tsvDf.seqLen, errors='coerce')
                tsvDf.exonCount = pd.to_numeric(tsvDf.exonCount, errors='coerce')
                
                trCount = len(tsvDf["contig"])
                
                avgLength = tsvDf["seqLen"].mean()
                medLength = tsvDf["seqLen"].median()
                minLength = tsvDf["seqLen"].min()
                maxLength = tsvDf["seqLen"].max()
                
                avgExonCount = tsvDf["exonCount"].mean()
                medExonCount = tsvDf["exonCount"].median()
                minExonCount = tsvDf["exonCount"].min()
                maxExonCount = tsvDf["exonCount"].max()
                
                
                noEffect = "NA"
                diffIntr = "NA"
                APAL = "NA"
                APAS = "NA"
                ASSL = "NA"
                ASSS = "NA"
                notInCont = "NA"
                if args.doCompare:
                    try:
                        noEffect = len(tsvDf[tsvDf.Result == "no_effect"].Result)
                        diffIntr = len(tsvDf[tsvDf.Result == "diffIntronUse"].Result)
                        APAL = len(tsvDf[tsvDf.Result == "APA_L"].Result)
                        APAS = len(tsvDf[tsvDf.Result == "APA_S"].Result)
                        ASSL = len(tsvDf[tsvDf.Result == "ASS_L"].Result)
                        ASSS = len(tsvDf[tsvDf.Result == "ASS_S"].Result)
                        notInCont = len(tsvDf[tsvDf.Result == "not_in_controll"].Result)
                    except:
                        pass
                
                try:
                    stDevLength = statistics.stdev(tsvDf["seqLen"])
                    stDevECount = statistics.stdev(tsvDf["exonCount"])
                except:
                    print("Only one datapoint.")
                    stDevLength = 0
                    stDevECount = 0
                with open("".join((args.out_path, "TrStat.tsv")), 'a') as statfile:
                    statfile.write("\t".join((filename.strip("_trLen.tsv"), str(trCount), 
                                           str(avgLength), str(medLength), str(minLength),
                                           str(maxLength), str(stDevLength),
                                           str(avgExonCount), str(medExonCount),
                                           str(minExonCount), str(maxExonCount),
                                           str(stDevECount), str(noEffect),
                                           str(diffIntr), str(APAL), 
                                           str(APAS), str(ASSL), str(ASSS), 
                                           str(notInCont), "\n")))
                statfile.close()

def TrCompare(controlDf, filename):
    tsvDf = pd.DataFrame()
    tsvDf = pd.read_csv("".join((args.out_path, filename)), sep = "\t")
    controlContList = set(controlDf.contig)
    tsvContList = set(tsvDf.contig)
    #Keep only common contigs
    commonContList = [value for value in controlContList if value in tsvContList]
    
    print("Calculating intron, alternative polyadenylation and transcriptional start site usage in " + filename + "\n")
    
    #Check if the exon composition is the same
    for c in commonContList:
        for exComp in controlDf[controlDf.contig == c].exonComp:
            S = controlDf[controlDf.exonComp == exComp].start.tolist()
            E = controlDf[controlDf.exonComp == exComp].end.tolist()
            #print(tsvDf.start, "\n", ([S[0]-args.w, S[0]+args.w]), "\n", tsvDf.start.isin(list(range(S[0]-args.w,S[0]+args.w))))
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start.isin(list(range(S[0]-args.w, S[0]+args.w)))) 
                      & (tsvDf.end.isin(list(range(E[0]-args.w, E[0]+args.w)))),"Result"] = "no_effect"
    #Check if start or end coordinates are the same and if the cell in the Result column is empty
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start.isin(list(range(S[0]-args.w, S[0]+args.w))))
                      & (tsvDf.end.isin(list(range(E[0]-args.w, E[0]+args.w)))) & (tsvDf.Result.isna()),"Result"] = "diffIntronUse"
    
    #Alternative polyadenylation detection with longer UTR on + strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start.isin(list(range(S[0]-args.w, S[0]+args.w)))) 
                      & (tsvDf.end > E[0]+args.w) & (tsvDf.Result.isna()) & (tsvDf.strand == "+"),"Result"] = "APA_L"
    
    #Alternative polyadenylation detection with shorter UTR on + strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start.isin(list(range(S[0]-args.w, S[0]+args.w)))) 
                      & (tsvDf.end < E[0]-args.w) & (tsvDf.strand == "+") & (tsvDf.Result.isna()),"Result"] = "APA_S"
    
    #Alternative start site detection with longer UTR on + strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start < S[0]-args.w) 
                      & (tsvDf.end.isin(list(range(E[0]-args.w, E[0]+args.w)))) & (tsvDf.Result.isna()) & (tsvDf.strand == "+"),"Result"] = "ASS_L"
    
    #Alternative start site detection with shorter UTR on + strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start > S[0]+args.w) 
                      & (tsvDf.end.isin(list(range(E[0]-args.w, E[0]+args.w)))) & (tsvDf.strand == "+") & (tsvDf.Result.isna()),"Result"] = "ASS_S"
    
    
    #Alternative start site detection with longer UTR on - strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start.isin(list(range(S[0]-args.w, S[0]+args.w))))
                      & (tsvDf.end > E[0]+args.w) & (tsvDf.Result.isna()) & (tsvDf.strand == "-"),"Result"] = "ASS_L"
    
    #Alternative start site detection detection with shorter UTR on - strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start.isin(list(range(S[0]-args.w, S[0]+args.w)))) 
                      & (tsvDf.end < E[0]-args.w) & (tsvDf.strand == "-") & (tsvDf.Result.isna()),"Result"] = "ASS_S"
    
    #Alternative polyadenylation detection with longer UTR on - strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start < S[0]-args.w) 
                      & (tsvDf.end.isin(list(range(E[0]-args.w, E[0]+args.w)))) & (tsvDf.Result.isna()) & (tsvDf.strand == "-"),"Result"] = "APA_L"
    
    #Alternative polyadenylation detection with shorter UTR on - strand
            tsvDf.loc[(tsvDf.contig == c) & (tsvDf.start > S[0]+args.w) 
                      & (tsvDf.end.isin(list(range(E[0]-args.w, E[0]+args.w)))) & (tsvDf.strand == "-") & (tsvDf.Result.isna()),"Result"] = "APA_S"
    
    #Gene not detected in controll
    tsvDf.loc[(tsvDf.Result.isna()),"Result"] = "not_in_controll"
    
    tsvDf.to_csv("".join((args.out_path, filename)), mode='w', sep = "\t", index=False)
    #print(tsvDf)

def LenCalc(filename):
    gffDf = pd.DataFrame(columns=gffCols)
    gffDf = pd.read_csv("".join((args.input_path, filename)),
                           names=gffCols, sep = "\t", comment="#")

    print("Calculating transcript lengths in " + filename)
    
    gffDf["exonComp"] = gffDf["exonComp"].astype("object")
    
    #Clarify transcript IDs and parents
    nameList = gffDf["name"].str.split(";")
    gffDf.id = list(item[0] for item in nameList)
    nameList = gffDf["id"].str.split("=")
    gffDf.id = list(item[1] for item in nameList)
    
    nameList = gffDf["name"].str.split(";")
   
    gffDf.parent = list(item[1] for item in nameList)
    nameList = gffDf["parent"].str.split("=")
    gffDf.parent = list(item[1] for item in nameList)
    
    #Calculate feature length and fetching exon composition.
    gffDf.seqLen = gffDf.end - gffDf.start
    gffDf.exonComp = list(zip(gffDf.start, gffDf.end))
    gffDf.exonCount = 1 
    
    #Sum exon len for spliced transcripts and merge exon composition lists.
    parSet = set(gffDf.parent)
    
    for p in parSet:
        gffDf.loc[gffDf.id == p, "parent"] = ""
        if p in (gffDf.parent).tolist():
            gffDf.loc[gffDf.id == p, "seqLen"] = sum(gffDf[gffDf.parent == p].seqLen.tolist())+1
            gffDf.loc[gffDf.id == p, "exonCount"] = len(gffDf[gffDf.parent == p].index)
            gffDf.loc[gffDf.id == p, "exonComp"] = [set(e  for tup in tuple(gffDf[gffDf.parent == p].exonComp.tolist()) for e in tup)]
            gffDf.loc[gffDf.parent == p, "parent"] = ""
    
    #Keep only the mRNA annotations. 
    gffDf = gffDf[gffDf.feature == "mRNA"]
    gffDf = gffDf.reset_index(drop = True)
    gffDf.to_csv("".join((args.out_path, filename.split(".gff3")[0], "_trLen.tsv")), sep = "\t", index=False,
             columns = tsvCols)

def Main():
    processes = []
    gffFileNames = []
    tsvFileNames = []
    controlDf = pd.DataFrame()
    pool = Pool(processes = os.cpu_count()-1)
    
    DirManager()
    
    for files in os.walk(args.input_path):
        for filename in files[2]:
            gffFound = False
            if filename.endswith(".gff3"):
                gffFound = True
                gffFileNames.append(filename)
    
    if gffFound == False:
        print("\n\n No gff3 files found in ", args.input_path,"\n")
        pass
    else:    
        pool.map(LenCalc, gffFileNames)
        pool.close()
        pool.join()
    print("\n")
    tsvFound  = False
    controlFound = False
    pool = Pool(processes = os.cpu_count()-1)
    
    if args.doCompare:
        for files in os.walk(args.out_path):
            for filename in files[2]:
                if filename.endswith("_trLen.tsv"):
                    tsvFound = True
                    if filename.endswith("".join((args.suffix,"_trLen.tsv"))):
                        controlFound = True
                        controlHelper = pd.DataFrame()
                        controlHelper = pd.read_csv("".join((args.out_path, filename)),
                               sep = "\t")
                        controlDf = pd.concat([controlDf, controlHelper])
                    else:
                        tsvFileNames.append(filename)
        controlDf = controlDf.drop_duplicates(subset='exonComp', keep="first")
        controlDf.reset_index(drop = True)
    
        if tsvFound == False:
            print("\n\n No files found in ", args.out_path,".\n")
        else:
            if controlFound == False:
                print("\n\n No controll files found in ", args.out_path,".\n")
                pass
            else:
                pool.map(partial(TrCompare, controlDf), tsvFileNames)
                pool.close()
                pool.join()
        print("\n")
    
    with open("".join((args.out_path, "TrStat.tsv")), 'w') as statfile:
            statfile.write("\t".join(("Samples", "Transcript count", "Mean transcript length", 
                                           "Median transcript length", "Min transcript lenght",
                                           "Max transcript lenght", "SD transcript lenght",
                                           "Mean exon count", "Median exon count",
                                           "Min exon count", "Max exon count",
                                           "SD exon count", "Invariant transcript count",
                                           "Transcripts with differential intron usage",
                                           "Alternative polyadenylation - Long", 
                                           "Alternative polyadenylation - Short",
                                           "Alternative TSS - Long",
                                           "Alternative TSS - Short",
                                           "Not present in control", "\n")))
            statfile.close()
   
    Stat()

if __name__ == "__main__":
	Main()	